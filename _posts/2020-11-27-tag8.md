---
title: "Tag 8"
date: 2020-11-27
---

<h1>Nachtrag zu Metadaten modellieren und Schnittstellen nutzen</h1>

<h2>Diskussion der Aufgabe zur Anreicherung</h2>
Zu dem Zeitpunkt wurde die Anreicherungsaufgabe in OpenRefine besprochen.
Darüber werde ich hier nicht berichten, aber möchte auf meinen [Eintrag verweisen(https://sakura-72.github.io/my-bain-blog/2020/11/26/anreicherung.html], in welcher ich ein kurzes Kommentar zu meine Fehler noch eingegeben habe.

<h2>Validierung von XML</h2>

* Wir exportieren das Gesamtergebnis als XML in ein neues Verzeichnis.

Wenn man ein sauberes XML machen möchte, muss man es mit einem XML-Schema abgleichen z.B. mit dem MARC21-Schema der Library of Congress. Man kann mit dem Schema von der Library of Congress nicht unbedingt die Inhalte verifizieren, aber zumindest die Form. Die Validierung ist eine wichtige Grundlage.
Diese Validierung wird vom Dozent an dem Beispiel der XML Datei mit den DOAJ-Daten aus Open Refine, welche wir zusammen exportiert haben.

* Für die Validierung können Sie das Programm `xmllint` verwenden, das unter Ubuntu vorinstalliert ist.
* Zum Abgleich gegen das offizielle Schema von MARC21 laden wir dieses (XSD) zunächst herunter.

Hier ein Beispiel, wie man mit disem Programm nutzen kann, um die Datei zu validieren:
```bash
wget https://www.loc.gov/standards/marcxml/schema/MARC21slim.xsd
xmllint *.xml --noout --schema MARC21slim.xsd
```
Einerseits wird mit dem ersten Kommando anhand wget den Schema von der Library of Congress als Datei heruntergeladen. Hier ist es wichtig darauf zu achten, dass die Datei am gleichen Ort wie unsere DOAJ-Datei gespeichert wird. Hierfür hat Her Lohmeier den Ordner Download aufgewählt und somit diesen Befehl in der Shell eingegeben:
```bash
cd Downloads
```
Dann den zweiten oberen Befehl eingegeben, wobei hier statt *"*.xml"* den DOAJ-Dateiname eigegeben wird. Damit läuft die Validierung nur über die XML-Datei, welche wir auswählen, und nicht über alle XML-Dateaien. Den *--noout*-Befehl ist dafür da, dass den Inhalt der Datei nicht in der Shell ausgegeben wird (wir wollen ja nur eine Validierung). Zum Schluss den Befehl *--schema* und darauf folgend den Dateiname des Library of Congress Schemas, welches wir heruntergeladen haben.
```bash
xmllint *.xml --noout --schema MARC21slim.xsd
```
Schade! Es kommen einige Fehler, somit haben wir mit der DOAJ-Datei kein valides XML erzeugt. Das heisst, dass wir jetzt nach dem Fehler in einem Editor suchen müssen. Es scheint, dass den Feld 260 b nicht geschlossen worden ist: es fehlt den schliessenden Subfield *</subfeld>*. 
>Immerhin hat es den Vorteil, dass es uns aufzeigt, dass die Valisierung eine sinnvolle Sache ist. :)

!!! Da ein Bild wäre schön @Almira!!! :)

<h3> Exkurs: XML-Deklaration </h3>
Eine XML-Deklaration (wie eigentlich auch jede Deklaration, wie z.B. in einem XML-File) dient dazu, einen Programm den Format unsere Datei mitzuteilen. In diesem Fall, dass es sich bei einer Datei, um XML handelt. Dafür wird am Anfang der Datei wie folgt deklariert:
  ```
  <?xml version="1.0" encoding="utf-8" standalone="yes"?>
  ```
* Hier handelt es sich um einer XML-Datei der Version 1.0. mit der Zeichencodierung im Standard Unicode. 
* Auch entählt die Datei eine Dokumenttypdefinition (DTD). Das ist so in etwa ein Metadaten-Schema, welches aber nicht separat sondern im Dokument liegt und Auskunft über die Form der Datei gibt. Dies wird nach dem Element *standalone* definiert: entweder mit dem Wert *yes (in der Datei enthalten), no (es gibt ein Schema, aber liegt ausserhalb der Datei) oder (leer/weg)gelassen (heisst, das kein Schema beigelegt ist und die Datei an dem XML-Standard orientiert).

Was man sich merken sollte:
* Die Versionangabe ist pflicht, die Encoding-Angaben gehörten zur guten Praxis. Das leztes Element wird allerginds selten gebraucht.
* die Reihenfolge der Elemente einer Deklaration ist festgelegt.


  <h2>Weitere Tools zur Metadatentransformation</h2>
<h3> Zur Motivation </h3>

>Wir dürfen gerne das Interviews mit Kirsten Jeude anschauen, fall wir wissen wollen, wie das in der Praxis ist und auch wenn wir beruflich eventuell mit Metadaten arbeiten möchten.

Metadaten-Management in der Praxis, hier beim Leibniz-Informationszentrum Wirtschaft (ZBW) in Hamburg:
* Infoseite: <https://www.zbw.eu/de/ueber-uns/arbeitsschwerpunkte/metadaten/>
* Videointerview mit Kirsten Jeude: <https://www.youtube.com/watch?v=YwbRTDvt_sA>

<h3> Vergleich mit anderen Tools </h3>
Herr Lohmeier gibt zu OpenRefine und Alternative ein kurzer Überblick und Emfehlungen. Nach ihm eignet sich für den Anfang OpenRefine sehr gut. Wenn OpenRefine für uns zu generisch wäre und man sich zusäztlich mit Programieren - also ohne graphische Oberfläche - ausseinandersetzten wollen, dann kan man auf Alternative zugreifen. Catmandu benutzt Perl, eine etwa veraltete Programmiersprache - und eine slebstentwickelte Skriptsprache ([Fix](https://librecat.org/Catmandu/#fix-language)). Diese Software ist zu empfehlen, wenn man sich mit dem Thema tiefen auseinandersetzen möchte.
Metafacture arbeitet daran Fix zu implementieren.

* Merkmale von OpenRefine:
    * grafische Oberfläche: Transformationsergebnisse werden direkt sichtbar
    * Skriptsprachen (GREL, Jython, Clojure) für komplexe Transformationen
    * Schwerpunkt auf Datenanreicherung (Reconciliation)
* Alternative Software:
    * [Catmandu](https://librecat.org) (Perl)
    * [Metafacture](https://github.com/metafacture/metafacture-core) (Java)
    * [MarcEdit](https://marcedit.reeset.net) (für MARC21)
* Siehe auch: Prof. Magnus Pfeffer (2016): Open Source Software zur Verarbeitung und Analyse von Metadaten. Präsentation auf dem 6. Bibliothekskongress. <http://nbn-resolving.de/urn/resolver.pl?urn:nbn:de:0290-opus4-24490>

Note:
* Generell gilt, dass die passende Software anhand des Anwendungsfalls gewählt werden sollte.
* In der Praxis wird oft die Software verwendet, die schon gut beherrscht wird. Das macht es manchmal sehr umständlich.


<h2>Nutzung von JSON-APIs</h2>
Wir haben verschiedene Schnittstellen angeschaut, allerdings sehr bibliothek-/archivspezifisch: SRU oder OAI-PMH.
Normalereiweise im Web geben Programierschnittstellen [JSON](https://wiki.selfhtml.org/wiki/JSON) aus, dies ist das übliche Format für moderne Schnittstellen.  JSON ("JavaScript Object Notation") ist - so wie die Abkürzung es aussagt - ein Javaskript Datenformat, welches Ifnroamtionen in einer lesbaren Form speichert. JSON wird oft mit Ajax genutzt, um Informationen zwischen Client und Sever auszutauschen. 
Sowie auch XML lässt sich JSON im Browser anschauen und maschinell gut verarbeiten. 

<h3>Erstes Beispiel für API: lobid-gnd</h3>
Auf [lobid](https://lobid.org/gnd/api), kann man auf Datensätzen zugreifen, sie über die graphische Oberfläche konsultieren und diese auch in JSON Linked-Data herunterladen. Der Browser kann diese interpretieren, so dass wir die Informationen gut lesen können.
Bei der Autovervollständigung kann man z.B. einem Namen suchen und erhält eine ID, welche uns den Zugriff auf dem JSON ermöglicht. Diese Option kann unter anderem die Vervollständigung eines Katalog dienen.

<h3>Zweites Beispiel für API: scrAPIr</h3>
Als weiteres Tool gibt es [scrAPIr](https://scrapir.org), ein Projekt vom MIT, welches für Menschen erstellt worden ist, die nicht unbedingt Programmieren können. Mit scrAPIr können Daten von Websiten angeschaut und bezogen werden, welche dursch die APIs der jeweiligen Webseiten übernommen werden. Auch kann man unter *>_code* hat man die Möglichkeit zu sehen, wie Abfragen in Javascript und Python gemacht werden können, und sich somit mit dem Abfragen mit Programmiersprachen auseinanderzusetzen.

Beispiel Google Books: https://scrapir.org/data-management?api=Google_Books


<h2>Metadatenstandard LIDO>/h2>
LIDO steht für Lightweight Information Describing Objects und ist auf [CIDOC Conceptual Reference Model](http://www.cidoc-crm.org/) - kurz CRM - basiert. Es handelt sich dabei um einem XML-Standard zur Beschreibung von Kulturobjekten und ist somit in Museen sehr vebreitet. Wir im Modul erst jetzt behandelt, da dieser Metadaten-Standard ein wenig anders geartet ist als andere Standards, die wir bischer angeschaut haben.
  CIDOC ist sehr abstrakt und somit auch nicht wirklich für die Anwedung gedacht. Es ist ein Modell, welcher eher den Aufbau von Konzepte dient. Dieser definiert URI für Konzepte und Relationen.
  LIDO macht sich CIDOC zur Nutzung, um daraus eine Sprache – eine Terminologie – zu erstellen und Museumsobjekte zu beschreiben. Als Vorteil entspricht LIDO dem Linked Open Data-Paradigma, da anhand der URIs Entitäten und Beziehungen eindeutig und nachschlagbar sind. Im Vergleich zu andere Metadaten-Standards, die Dokumente beschreiben, beschreibt LIDO Objekte ereignis-zentriert (Ereignisse als Entitäten). Damit jedes Museumsobjekt ist mit Ereignisse verbunden: so wird seine Geschichte beschrieben und sein Kontext definiert z.B. wie es erstellt worden ist oder wie wurde daran etwas geändert. Dazu sind Individuen, die an diese Ereignisse teilgenommen haben, werden auch erschlossen. Diese Struktur ist sehr intreressant, allerdings entsteht dabei ein grosses Problem: eine verlustfreie Transformation zu andere Formate ist schwer umzusetzen.
  
<h3>Struktur</h3>
Bei LIDO sind deskriptive Metadaten, administrative Metadaten und administrative Daten enthalten.

* Deskriptive Metadaten:
Identifikation und Klassifikation, die wir aus Dokumentzentrierte Metadaten auch schon kennen. Es sind beschreibende Metadaten und sagen aus um was es sich handelt, wie die Identifikation mit Titel, Name oder Masse. Die Klassifikation ist ein wenig anders, weil sich auch wieder an CIDOC orientiert, so müssen URIs und den genormten Vokabular entsprechen. Hinzu kommt das zentrlaes Element der **Ereignisse**. Direkte Relatione zwischen Verschiedene Ressourcen.

* administrative Metadaten:

* administrative Daten:

  
>Feedback an Herr Meyer: Es tut mir leid, aber für mich sprechen Sie viel zu schnell, um zu folgen. 


<h1>Suchmaschinen und Discovery-Systeme 1/2 </h1>

<h2>Installation und Konfiguration von VuFind</h2>

<h2>Funktion von Suchmaschinen am Beispiel von Solr</h2>
    
<h1>Aufgaben</h1>
